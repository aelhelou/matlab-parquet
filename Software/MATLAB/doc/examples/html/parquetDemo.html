<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="generator" content="MATLAB R2018a"><meta http-equiv="X-UA-Compatible" content="IE=edge,IE=9,chrome=1"><title>MATLAB Interface for Parquet</title><style type="text/css">
* {margin: 0; padding: 0;}
body {text-align: start; line-height: 17.234001159668px; min-height: 0px; white-space: normal; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, 'Courier New', monospace; font-style: normal; font-size: 13.9999990463257px; font-weight: normal; text-decoration: none; white-space: normal; }
h1, h2 {font-weight: normal;}
.content { padding: 30px; }

.S0 { margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S1 { text-align: left; line-height: 26.3999977111816px; min-height: 24px; white-space: pre-wrap; color: rgb(213, 80, 0); font-family: Helvetica, Arial, sans-serif; font-size: 22px; white-space: pre-wrap; margin-left: 4px; margin-top: 3px; margin-bottom: 15px; margin-right: 10px;  }
.S2 { min-height: 0px; margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S3 { margin-left: 0px; margin-top: 0px; margin-bottom: 20px; margin-right: 0px;  }
.S4 { line-height: 21px; min-height: 17px; white-space: pre-wrap; font-family: Helvetica, Arial, sans-serif; font-weight: bold; white-space: pre-wrap; margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S5 { line-height: 21px; min-height: 17px; white-space: pre-wrap; font-family: Helvetica, Arial, sans-serif; white-space: pre-wrap; margin-left: 0px; margin-top: -1px; margin-bottom: 0px; margin-right: 0px;  }
.S6 { min-height: 0px; color: rgb(0, 95, 206); margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S7 { text-align: left; line-height: 20.576000213623px; min-height: 20px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-size: 16px; font-weight: bold; white-space: pre-wrap; margin-left: 4px; margin-top: 15px; margin-bottom: 9px; margin-right: 10px;  }
.S8 { text-align: left; line-height: 21px; min-height: 17px; white-space: pre-wrap; font-family: Helvetica, Arial, sans-serif; white-space: pre-wrap; margin-left: 4px; margin-top: 2px; margin-bottom: 9px; margin-right: 10px;  }
.S9 { font-family: Menlo, Monaco, Consolas, 'Courier New', monospace; margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S10 { font-family: Helvetica, Arial, sans-serif; margin-left: 0px; margin-top: 10px; margin-bottom: 20px; margin-right: 0px;  }
.S11 { text-align: left; line-height: 20.9999980926514px; white-space: pre-wrap; white-space: pre-wrap; margin-left: 55.9999961853027px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S12 { text-align: left; line-height: 20.576000213623px; min-height: 20px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-size: 16px; font-weight: bold; white-space: pre-wrap; margin-left: 4px; margin-top: 3px; margin-bottom: 9px; margin-right: 10px;  }
.S13 { margin-left: 3px; margin-top: 10px; margin-bottom: 4px; margin-right: 3px;  }
.S14 { min-height: 18px; white-space: nowrap; white-space: nowrap; margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S15 { min-height: 0px; white-space: pre; white-space: pre; margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S16 { color: rgb(160, 32, 240); margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }
.S17 { margin-left: 3px; margin-top: 10px; margin-bottom: 10px; margin-right: 3px;  }
.S18 { text-align: left; line-height: 21px; min-height: 17px; white-space: pre-wrap; font-family: Helvetica, Arial, sans-serif; white-space: pre-wrap; margin-left: 4px; margin-top: 10px; margin-bottom: 9px; margin-right: 10px;  }
.S19 { color: rgb(0, 0, 255); margin-left: 0px; margin-top: 0px; margin-bottom: 0px; margin-right: 0px;  }

.CodeBlock {margin: 10px 0 10px 0; background-color: #F7F7F7;}
.CodeBlock+.paragraphNode {margin-top: 10px;}
.lineNode {padding-left: 10px; border-left: 1px solid #E9E9E9; border-right: 1px solid #E9E9E9;}
.inlineWrapper:first-child .lineNode,.inlineWrapper.outputs+.inlineWrapper .lineNode {padding-top: 5px; border-top: 1px solid #E9E9E9;}
.inlineWrapper:last-child .lineNode,.inlineWrapper.outputs .lineNode {padding-bottom: 5px; border-bottom: 1px solid #E9E9E9;}
.lineNode .textBox {white-space: pre;}
</style></head><body><div class = "content"><div class = 'SectionBlock containment active'><h1 class = "S1"><span class = "S2"><span class="S0" id="T_ECDFA682" >MATLAB Interface for Parquet</span></span></h1><div class = "S3"><div class = "S4"><span class = "S2"><span class="S0">Table of Contents</span></span></div><div class = "S5"><a href = "#H_C6B1F8FD"><span class = "S0"><span class="S0">Introduction<br></span></span></a><a href = "#H_5337F9DF"><span class = "S0"><span class="S0">Write a Parquet file<br></span></span></a><a href = "#H_D1A535EA"><span class = "S0"><span class="S0">Parquet file information<br></span></span></a><a href = "#H_51EEBC22"><span class = "S0"><span class="S0">Reading in a Parquet file<br></span></span></a><a href = "#H_8F5E62B7"><span class = "S0"><span class="S0">Parquet datastore<br></span></span></a><a href = "#H_62215F9E"><span class = "S0"><span class="S0">Parquet datastore and tall arrays<br></span></span></a><a href = "#H_B5C09AC9"><span class = "S0"><span class="S0">Parquet tall arrays with Parallel Computing Toolbox<br></span></span></a><a href = "#H_EEB22528"><span class = "S0"><span class="S0">Parquet tall arrays with Spark/Hadoop cluster and MDCS</span></span></a></div></div><h2 class = "S7"><span class = "S2"><span class="S0" id="H_C6B1F8FD" >Introduction</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">The two main function for reading and writing Parquet files are wrappers around respective Reader and Writer classes. As such the functions take as input optional Property/Value pairs which are public properties of the above classes. parquetDatastore</span></span><span class = "S2"><span class="S9"> </span></span></div><ul class = "S10"><li class = "S11"><span class = "S0"><span class="S9">parquetwrite     - </span></span><span class = "S0"><span class="S0">Write MATLAB data to parquet file</span></span></li><li class = "S11"><span class = "S0"><span class="S9">parquetread      - </span></span><span class = "S0"><span class="S0">Read in a parquet file</span></span></li><li class = "S11"><span class = "S0"><span class="S9">parquetDatastore - </span></span><span class = "S0"><span class="S0">Create a datastore to a collection of parquet files for use with tall arrays</span></span></li></ul><div class = "S8"><span class = "S2"><span class="S0">We also have utility functions</span></span></div><ul class = "S10"><li class = "S11"><span class = "S0"><span class="S9">parquetinfo</span></span><span class = "S0"><span class="S0">   - Returns the meta information of the Parquet file as a structure</span></span></li><li class = "S11"><span class = "S0"><span class="S9">parquettools</span></span><span class = "S0"><span class="S0"> - Runs </span></span><span class = "S0"><span class="S9">parquet-tools</span></span></li></ul></div><div class = "S0"></div><div class = 'SectionBlock containment'><h2 class = "S12"><span class = "S2"><span class="S0" id="H_5337F9DF" >Write a Parquet file</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">First create a </span></span><span class = "S2"><span class="S9">timetable</span></span><span class = "S2"><span class="S0"> of daily-stock returns for a portfolio of ten stocks for a year. </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">days = datetime(2000,1,1) : datetime(2000,12,31); </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">n = length(days); </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">s = 10; </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">r = cumsum(randn(n,s));</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">data = array2timetable(r,</span><span class="S16">'RowTimes'</span><span class="S0">,days');</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">data.News = </span><span class="S16">"News_" </span><span class="S0">+ (1 : n)';</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">Write this data to tmp.parquet, and return the optional Writer class</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">writer = parquetwrite(</span><span class="S16">'tmp.parquet'</span><span class="S0">,data)</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">This uses the default </span></span><span class = "S2"><span class="S9">Writer</span></span><span class = "S2"><span class="S0"> options, which we can use explicitly by calling the </span></span><span class = "S2"><span class="S9">bigdata.parquet.Writer</span></span><span class = "S2"><span class="S0"> class, or we can return the </span></span><span class = "S2"><span class="S9">Writer</span></span><span class = "S2"><span class="S0"> in the above example by specifying it as an output to </span></span><span class = "S2"><span class="S9">parquetwrite</span></span><span class = "S2"><span class="S0"> with or without input arguments.</span></span></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">To get help at any time on an object remember we can do </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">doc </span><span class="S16">writer</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0"> or </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">help </span><span class="S16">writer</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">To get specific help about a method or property such as the RowGroupSize, use the syntax </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">help </span><span class="S16">writer/RowGroupSize</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">By default you can see that we use Snappy compression with DictionaryEncoding, if we wanted to try Gzip option with no DictionaryEncoding. </span></span></div><div class = "S8"><span class = "S2"><span class="S0">You can use TAB auto-completion after the '=' to see the valid options: </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">writer.CompressionCodec   = </span><span class="S16">'GZIP'</span><span class="S0">; </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">writer.DictionaryEncoding = </span><span class="S16">'FALSE'</span><span class="S0">; </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">writer.WriteMode          = </span><span class="S16">'OVERWRITE'</span><span class="S0">; </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">writer.write </span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">Using the function form we can do this all with </span></span><span class = "S2"><span class="S9">parquetwrite:</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">parquetwrite(</span><span class="S16">'tmp.parquet'</span><span class="S0">,data,</span><span class="S16">'CompressionCodec'</span><span class="S0">,</span><span class="S16">'GZIP'</span><span class="S0">,</span><span class="S16">'DictionaryEncoding'</span><span class="S0">,</span><span class="S16">'FALSE'</span><span class="S0">,</span><span class="S16">'WriteMode'</span><span class="S0">,</span><span class="S16">'OVERWRITE'</span><span class="S0">)</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><h2 class = "S12"><span class = "S2"><span class="S0" id="H_D1A535EA" >Parquet file information</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">We can read in detailed information about the Parquet file as a structure</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">parquetinfo(</span><span class="S16">'tmp.parquet'</span><span class="S0">)</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">Similarly one could have run:</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">parquettools(</span><span class="S16">'meta'</span><span class="S0">,</span><span class="S16">'tmp.parquet'</span><span class="S0">)</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><h2 class = "S12"><span class = "S2"><span class="S0" id="H_51EEBC22" >Reading in a Parquet file</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">Now lets read in the data, and also return the </span></span><span class = "S2"><span class="S9">Reader</span></span><span class = "S2"><span class="S0"> class:</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">[read_data, reader] = parquetread(</span><span class="S16">'tmp.parquet'</span><span class="S0">); </span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">The reader can also support optional Property/Value pairs and we can see what they are by looking at the </span></span><span class = "S2"><span class="S9">reader</span></span><span class = "S2"><span class="S0"> value</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">reader</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">We can specify only a subset of fields to fetch from the Parquet file and limit it to a certain number of rows to read as can be seen from the above properties.</span></span></div><div class = "S8"><span class = "S2"><span class="S0">To verify data read in is the same as what we originally started with in MATLAB:</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">assert(isequaln(read_data, data))</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><h2 class = "S12"><span class = "S2"><span class="S0" id="H_8F5E62B7" >Parquet datastore</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">In this example we will generate some files and then use a custome datastore to process the data using tall and a parallel pool of workers.</span></span></div><div class = "S8"><span class = "S2"><span class="S0">First create 20 Parquet files with one-million rows and two columns each of random data</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">delete(</span><span class="S16">'*.parquet'</span><span class="S0">)</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">arrayfun(@(x) parquetwrite(</span><span class="S16">"tmp" </span><span class="S0">+ x + </span><span class="S16">".parquet"</span><span class="S0">,randn(1e6,2)), 1 : 20)</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">Using the </span></span><span class = "S2"><span class="S9">ParquetDatastore </span></span><span class = "S2"><span class="S0">we can easily perform analytics using tall arrays on a collection of Parquet files. </span></span></div><div class = "S8"><span class = "S2"><span class="S0">First create the datastore for all files in the current folder with file extension </span></span><span class = "S2"><span class="S9">.parquet </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">ds = parquetDatastore(</span><span class="S16">'tmp*.parquet'</span><span class="S0">)</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">Lets compute the mean by reading in all the data, since we have written a numeric array</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">tic </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">mean(ds.readall); </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">toc </span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">You would not want to do this if the amount of data you have is much greater than your system memory. In that case you would want to use a tall array.</span></span></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><h2 class = "S12"><span class = "S2"><span class="S0" id="H_62215F9E" >Parquet datastore and tall arrays</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">We can reuse the above datastore and create a tall array out of it</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">t = tall(ds);</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">tic</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">gather(mean(t));</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">toc</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><h2 class = "S12"><span class = "S2"><span class="S0" id="H_B5C09AC9" >Parquet tall arrays with Parallel Computing Toolbox</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">We can also run this in parallel on the local machine by using </span></span><span class = "S2"><span class="S9">parpool</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">parpool</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">t = tall(ds);</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">The first time we run a tall after starting the parpool the run-time will be slower due to initialization tasks. Subsequent runs will be much faster.</span></span></div><div class = "S8"><span class = "S2"><span class="S0">Time to run using our local worker pool: </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">tic </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">gather(mean(t));</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">toc</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">Lets run this again now that our pool is warmed up</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">tic </span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">gather(mean(t));</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">toc</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">Thus we can see that there is quite an improvement in reading Parquet files and performing analytics using a tall arrays in parallel versus using a serial datastore approach.</span></span></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><h2 class = "S12"><span class = "S2"><span class="S0" id="H_EEB22528" >Parquet tall arrays with Spark/Hadoop cluster and MDCS</span></span></h2><div class = "S8"><span class = "S2"><span class="S0">For this example we will run the same tall job with Parquet files using Spark executors and MDCS. </span></span></div><div class = "S8"><span class = "S2"><span class="S0">First we set our environment variables for Hadoop, in this case we are using Cloudera CDH 5.14.</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">setenv(</span><span class="S16">'HADOOP_PREFIX'</span><span class="S0">, </span><span class="S16">'/opt/cloudera/parcels/CDH/'</span><span class="S0">);</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">setenv(</span><span class="S16">'SPARK_HOME'</span><span class="S0">,    </span><span class="S16">'/opt/cloudera/parcels/CDH/lib/spark'</span><span class="S0">);</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">Create our temporary files again if we have deleted them:</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S19">if </span><span class="S0">isempty(dir(</span><span class="S16">'tmp*.parquet'</span><span class="S0">))</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">    arrayfun(@(x) parquetwrite(</span><span class="S16">"tmp" </span><span class="S0">+ x + </span><span class="S16">".parquet"</span><span class="S0">,randn(1e6,2)), 1 : 20)</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S19">end</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">Copy over our files to a temporary folder on HDFS</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">system(</span><span class="S16">'hdfs dfs -mkdir -p /user/ubuntu/data/parquet-temp'</span><span class="S0">);</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">system(</span><span class="S16">'hdfs dfs -rm -f /user/ubuntu/data/parquet-temp/*.parquet'</span><span class="S0">);</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">system(</span><span class="S16">'hdfs dfs -put *.parquet /user/ubuntu/data/parquet-temp'</span><span class="S0">);</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">Setup our datastore pointing to our data on HDFS</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">URI = </span><span class="S16">'hdfs://170.31.1.100:8020'</span><span class="S0">;</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">ds  = parquetDatastore([URI,</span><span class="S16">'/user/ubuntu/data/parquet-temp'</span><span class="S0">]);</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">A quick way to discover your URI is:</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">URI = parquetDatastore(</span><span class="S16">'-getHDFSHome'</span><span class="S0">);</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">Define execution environment for our cluster</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">cluster = parallel.cluster.Hadoop;</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">cluster.ClusterMatlabRoot = </span><span class="S16">'/usr/local/MATLAB/R2018a'</span><span class="S0">;</span></span></div></div><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">mr = mapreducer(cluster);</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">Path to our Parquet code on each node </span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">mr.Cluster.AdditionalPaths = </span><span class="S16">'/home/ubuntu/code'</span></span></div></div></div></div><div class = "S0"></div><div class = 'SectionBlock containment'><div class = "S8"><span class = "S2"><span class="S0">Create tall array</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">t = tall(ds);</span></span></div></div></div><div class = "S18"><span class = "S2"><span class="S0">Find the mean</span></span></div><div class = 'CodeBlock contiguous'><div class = 'inlineWrapper'><div class = "S14 lineNode"><span class = "S15"><span class="S0">gather(mean(t))</span></span></div></div></div></div></div><br><!-- <br>##### SOURCE BEGIN #####<br>%% MATLAB Interface for Parquet<br>%% Introduction<br>% The two main function for reading and writing Parquet files are wrappers around <br>% respective Reader and Writer classes. As such the functions take as input optional <br>% Property/Value pairs which are public properties of the above classes. parquetDatastore| <br>% |<br>% <br>% * |parquetwrite     - |Write MATLAB data to parquet file<br>% * |parquetread      - |Read in a parquet file<br>% * |parquetDatastore - |Create a datastore to a collection of parquet files <br>% for use with tall arrays<br>% <br>% We also have utility functions<br>% <br>% * |parquetinfo|   - Returns the meta information of the Parquet file as a <br>% structure<br>% * |parquettools| - Runs |parquet-tools|<br>%% Write a Parquet file<br>% First create a |timetable| of daily-stock returns for a portfolio of ten stocks <br>% for a year. <br>%%<br>days = datetime(2000,1,1) : datetime(2000,12,31); <br>n = length(days); <br>s = 10; <br>r = cumsum(randn(n,s));<br>data = array2timetable(r,'RowTimes',days');<br>data.News = "News_" + (1 : n)';<br>%% <br>% Write this data to tmp.parquet, and return the optional Writer class<br>%%<br>writer = parquetwrite('tmp.parquet',data)<br>%% <br>% This uses the default |Writer| options, which we can use explicitly by <br>% calling the |bigdata.parquet.Writer| class, or we can return the |Writer| in <br>% the above example by specifying it as an output to |parquetwrite| with or without <br>% input arguments.<br>% <br>% To get help at any time on an object remember we can do <br>%%<br>doc writer<br>%% <br>%  or <br><br>help writer<br>%% <br>% To get specific help about a method or property such as the RowGroupSize, <br>% use the syntax <br><br>help writer/RowGroupSize<br>%% <br>% By default you can see that we use Snappy compression with DictionaryEncoding, <br>% if we wanted to try Gzip option with no DictionaryEncoding. <br>% <br>% You can use TAB auto-completion after the '=' to see the valid options: <br>%%<br>writer.CompressionCodec   = 'GZIP'; <br>writer.DictionaryEncoding = 'FALSE'; <br>writer.WriteMode          = 'OVERWRITE'; <br>writer.write <br>%% <br>% Using the function form we can do this all with |parquetwrite:|<br>%%<br>parquetwrite('tmp.parquet',data,'CompressionCodec','GZIP','DictionaryEncoding','FALSE','WriteMode','OVERWRITE')<br>%% Parquet file information<br>% We can read in detailed information about the Parquet file as a structure<br>%%<br>parquetinfo('tmp.parquet')<br>%% <br>% Similarly one could have run:<br><br>parquettools('meta','tmp.parquet')<br>%% Reading in a Parquet file<br>% Now lets read in the data, and also return the |Reader| class:<br>%%<br>[read_data, reader] = parquetread('tmp.parquet'); <br>%% <br>% The reader can also support optional Property/Value pairs and we can see <br>% what they are by looking at the |reader| value<br><br>reader<br>%% <br>% We can specify only a subset of fields to fetch from the Parquet file <br>% and limit it to a certain number of rows to read as can be seen from the above <br>% properties.<br>% <br>% To verify data read in is the same as what we originally started with in <br>% MATLAB:<br><br>assert(isequaln(read_data, data))<br>%% *Parquet datastore*<br>% In this example we will generate some files and then use a custome datastore <br>% to process the data using tall and a parallel pool of workers.<br>% <br>% First create 20 Parquet files with one-million rows and two columns each <br>% of random data<br>%%<br>delete('*.parquet')<br>arrayfun(@(x) parquetwrite("tmp" + x + ".parquet",randn(1e6,2)), 1 : 20)<br>%% <br>% Using the |ParquetDatastore |we can easily perform analytics using tall <br>% arrays on a collection of Parquet files. <br>% <br>% First create the datastore for all files in the current folder with file <br>% extension |.parquet |<br><br>ds = parquetDatastore('tmp*.parquet')<br>%% <br>% Lets compute the mean by reading in all the data, since we have written <br>% a numeric array<br>%%<br>tic <br>mean(ds.readall); <br>toc <br>%% <br>% You would not want to do this if the amount of data you have is much greater <br>% than your system memory. In that case you would want to use a tall array.<br>%% Parquet datastore and tall arrays<br>% We can reuse the above datastore and create a tall array out of it<br>%%<br>t = tall(ds);<br>tic<br>gather(mean(t));<br>toc<br>%% Parquet tall arrays with Parallel Computing Toolbox<br>% We can also run this in parallel on the local machine by using |parpool|<br>%%<br>parpool<br>t = tall(ds);<br>%% <br>% The first time we run a tall after starting the parpool the run-time will <br>% be slower due to initialization tasks. Subsequent runs will be much faster.<br>% <br>% Time to run using our local worker pool: <br>%%<br>tic <br>gather(mean(t));<br>toc<br>%% <br>% Lets run this again now that our pool is warmed up<br>%%<br>tic <br>gather(mean(t));<br>toc<br>%% <br>% Thus we can see that there is quite an improvement in reading Parquet <br>% files and performing analytics using a tall arrays in parallel versus using <br>% a serial datastore approach.<br>%% Parquet tall arrays with Spark/Hadoop cluster and MDCS<br>% For this example we will run the same tall job with Parquet files using Spark <br>% executors and MDCS. <br>% <br>% First we set our environment variables for Hadoop, in this case we are <br>% using Cloudera CDH 5.14.<br>%%<br>setenv('HADOOP_PREFIX', '/opt/cloudera/parcels/CDH/');<br>setenv('SPARK_HOME',    '/opt/cloudera/parcels/CDH/lib/spark');<br>%% <br>% Create our temporary files again if we have deleted them:<br>%%<br>if isempty(dir('tmp*.parquet'))<br>    arrayfun(@(x) parquetwrite("tmp" + x + ".parquet",randn(1e6,2)), 1 : 20)<br>end<br>%% <br>% Copy over our files to a temporary folder on HDFS<br><br>system('hdfs dfs -mkdir -p /user/ubuntu/data/parquet-temp');<br>system('hdfs dfs -rm -f /user/ubuntu/data/parquet-temp/*.parquet');<br>system('hdfs dfs -put *.parquet /user/ubuntu/data/parquet-temp');<br>%% <br>% Setup our datastore pointing to our data on HDFS<br>%%<br>URI = 'hdfs://170.31.1.100:8020';<br>ds  = parquetDatastore([URI,'/user/ubuntu/data/parquet-temp']);<br>%% <br>% A quick way to discover your URI is:<br><br>URI = parquetDatastore('-getHDFSHome');<br>%% <br>% Define execution environment for our cluster<br><br>cluster = parallel.cluster.Hadoop;<br>cluster.ClusterMatlabRoot = '/usr/local/MATLAB/R2018a';<br>mr = mapreducer(cluster);<br>%% <br>% Path to our Parquet code on each node <br><br>mr.Cluster.AdditionalPaths = '/home/ubuntu/code'<br>%% <br>% Create tall array<br>%%<br>t = tall(ds);<br>%% <br>% Find the mean<br><br>gather(mean(t))<br>##### SOURCE END #####<br>--></body></html>